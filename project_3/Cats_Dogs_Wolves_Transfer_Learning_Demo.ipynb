{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "-OfGhW-ZUG9Q",
    "outputId": "b8e33e55-9143-469a-903d-4994b006d2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 2080 Ti (UUID: GPU-b3420b31-4832-b27f-df3a-6215a94894aa)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --list-gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Tensorflow version: 2.5.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6534851809960915102\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9392422912\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 122676046550371981\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:0b:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Cn6KlS01Y1r"
   },
   "source": [
    "### Downloading the Data\n",
    "Ensure that you have uploaded the dataset to the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EgzGQwrNGOsJ",
    "outputId": "6728b743-ba90-4145-abe2-f8620d70dde0"
   },
   "outputs": [],
   "source": [
    "# !python -m pip install gdown --quiet\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"animals.zip\"):\n",
    "    import gdown\n",
    "    url = 'https://drive.google.com/uc?id=18pulZkIQzDHGkH-UrDzxHp_giSxerTag'\n",
    "    # url = 'https://drive.google.com/uc?id=1ImrsoqvQ3JC-iGW2znQmfkoZyL5E3icJ'\n",
    "    # url = 'https://drive.google.com/uc?id=1S-OJVgxKXDeS5gmqJwzUaCpXmuT4zWcu'\n",
    "    output = 'animals.zip'\n",
    "    gdown.download(url, output, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5ndA88KR3zY-",
    "outputId": "72d36400-9fde-473a-be14-794ce45b0d6f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unzip files if they don't exist\n",
    "\"\"\"\n",
    "if not os.path.exists(\"cats-dogs-wolves-\"):\n",
    "    !unzip animals.zip\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "_9zS58LoX63h",
    "outputId": "a4648374-7a16-43d3-b0e1-d8f691b2810e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths Loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access denied - .\n",
      "File not found - -NAME\n",
      "File not found - -TYPE\n",
      "File not found - F\n",
      "File not found - -DELETE\n"
     ]
    }
   ],
   "source": [
    "!find . -name \".DS_Store\" -type f -delete\n",
    "\n",
    "train_path = 'cats-dogs-wolves-/datasets'\n",
    "test_path = 'cats-dogs-wolves-/testData'\n",
    "print(\"Paths Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqQejRNKmCGR"
   },
   "source": [
    "### Importing the Software Libraries\n",
    "\n",
    "The following code imports some software libraries necessary to execute the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "WQGet9FW-n55",
    "outputId": "7b030a67-75ed-45df-a892-1995a5610235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Success!\n"
     ]
    }
   ],
   "source": [
    "# If in colab\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    %tensorflow_version 2.x  # Uncomment for colab\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Nadam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from mlxtend.evaluate import confusion_matrix  # Use this if using colab I think\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "print(\"Import Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 24\n",
    "\n",
    "BATCH_SIZE_TRAIN = 15\n",
    "BATCH_SIZE_VALIDATE = 8\n",
    "BATCH_SIZE_TEST = 133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "iojaSuJW-pJw",
    "outputId": "f1961627-a348-4f33-d745-cfc1b50cfe11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 362 images belonging to 3 classes.\n",
      "Found 0 images belonging to 3 classes.\n",
      "Found 133 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = (224, 224)\n",
    "classes = ['cats', 'dogs', 'wolves']\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # validation_split=0.20,  # set validation split\n",
    "    validation_split=0,\n",
    ") \n",
    "\n",
    "train_batches = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    image_size,\n",
    "    classes=classes,\n",
    "    batch_size=BATCH_SIZE_TRAIN,\n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "valid_batches = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    image_size,\n",
    "    classes=classes,\n",
    "    batch_size=BATCH_SIZE_VALIDATE,\n",
    "    subset='validation',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(\n",
    "    test_path,\n",
    "    image_size,\n",
    "    classes=classes,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    seed=SEED\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "dFCcMRb_jw5h",
    "outputId": "59053161-c37c-4da6-f59d-d322a678b032"
   },
   "outputs": [],
   "source": [
    "vgg16_model = tensorflow.keras.applications.VGG16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUiLZgsPip5k"
   },
   "source": [
    "### Creating a Custom Output Model#\n",
    "\n",
    "A new model is created copying all layers except the output layer. Notice that these layers are set to be non-trainable. Finally, a custom output layer is added to the model.  This output layer should have the same number of outputs as the number of different classes of data. The softmax activation function conditions the output to be a probability distribuiton adding up to unity. Therefore, our model will output a probability distribution among the classes as output for each input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1        relu      None                                                                  False\n",
      "block1_conv2        relu      None                                                                  False\n",
      "block1_pool         None      None                                                                  False\n",
      "block2_conv1        relu      None                                                                  False\n",
      "block2_conv2        relu      None                                                                  False\n",
      "block2_pool         None      None                                                                  False\n",
      "block3_conv1        relu      None                                                                  False\n",
      "block3_conv2        relu      None                                                                  False\n",
      "block3_conv3        relu      None                                                                  False\n",
      "block3_pool         None      None                                                                  False\n",
      "block4_conv1        relu      None                                                                  False\n",
      "block4_conv2        relu      None                                                                  False\n",
      "block4_conv3        relu      None                                                                  False\n",
      "block4_pool         None      None                                                                  False\n",
      "block5_conv1        relu      None                                                                  False\n",
      "block5_conv2        relu      None                                                                  False\n",
      "block5_conv3        relu      None                                                                  False\n",
      "block5_pool         None      None                                                                  False\n",
      "flatten             None      None                                                                  False\n",
      "fc1                 relu      None                                                                  False\n",
      "fc2                 relu      None                                                                  False\n",
      "dense               elu       None                                                                  True\n",
      "dense_1             softmax   None                                                                  True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 142,657,347\n",
      "Trainable params: 8,396,803\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Make a new model\n",
    "model = Sequential()\n",
    "\n",
    "# Copy every layer except the last layer of the vgg16 model\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    model.add(layer)\n",
    "\n",
    "\"\"\"\n",
    "Set the layers in the new model to not trainable\n",
    "\n",
    "Notes:\n",
    "    Recall that layers be default are trainable\n",
    "    Retraining the last 2 layers, fc1 and fc2, is not a good idea unless you have a lot of data\n",
    "\"\"\"\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "# Additional layer if needed\n",
    "model.add(Dense(2048,\n",
    "                activation='elu',\n",
    "                # activity_regularizer=regularizers.l2(.05)  # Regularization seems like a bad idea since we still don't get good results\n",
    "                )\n",
    "          )\n",
    "\n",
    "# model.add(Dense(2048,\n",
    "#                 activation='relu',\n",
    "#                 activity_regularizer=regularizers.l2(1e-4)\n",
    "#                 )\n",
    "#           )\n",
    "\n",
    "# Add a new dense layer (This layer will be trained)\n",
    "model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(\"{:<20}{:<10}{:<70}{}\".format(layer.name,\n",
    "                                        str(layer.get_config().get('activation')),\n",
    "                                        str(layer.get_config().get('activity_regularizer')),\n",
    "                                        layer.trainable)\n",
    "          )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQsSiXUk_j0J"
   },
   "source": [
    "## Training the Model\n",
    "\n",
    "Now that the model has been specified, it must be trained. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI8ljOkmpa0v"
   },
   "source": [
    "### Compiling the Model\n",
    "First,the model is compiled. The performance of the model is evalued using a loss function, which increases as performance decreases. Since we are predicting a condition (yes/no), we are using cross-entropy, which is a concept from information theory. If we were instead predicting a scalar value, the correct loss function would be 'mean_squared_error'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "iDG0uQ1qAccz",
    "outputId": "6968697c-1dfd-46f2-e89a-4950fcce4494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "model.compile(Adam(learning_rate=.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df0KalV7pnmx"
   },
   "source": [
    "### Minimizing Loss\n",
    "\n",
    "The model is now trained using the backpropagation algorithm to minimize the loss function. The training data will be passed into the model and evaluated using the loss function. Then, the model will be adjusted to decrease the loss.\n",
    "\n",
    "Here we specifiy the epochs, which is the number of times we pass through the entire training set. These specifications are determined by experimentation with the goal of minimizing validation loss. If the epochs are too high, the model has overfit to the data (too specific). If the epochs are too low, the model has underfit to the data (too general). The validation loss must be compared to training loss to determine if the model is optimally fit to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tkFe_2sRpnNt",
    "outputId": "6aa93857-ff0c-4db4-bb51-2e66e1a92498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "24/24 [==============================] - 7s 82ms/step - loss: 0.3889 - accuracy: 0.9135\n",
      "Epoch 2/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 0.0249 - accuracy: 0.9885\n",
      "Epoch 3/200\n",
      "24/24 [==============================] - 2s 68ms/step - loss: 0.0058 - accuracy: 0.9971\n",
      "Epoch 4/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 5/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 9.4266e-05 - accuracy: 1.0000\n",
      "Epoch 6/200\n",
      "24/24 [==============================] - 1s 63ms/step - loss: 1.5923e-04 - accuracy: 1.0000\n",
      "Epoch 7/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 9.8974e-05 - accuracy: 1.0000\n",
      "Epoch 8/200\n",
      "24/24 [==============================] - 2s 61ms/step - loss: 6.0335e-05 - accuracy: 1.0000\n",
      "Epoch 9/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 5.8162e-05 - accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "24/24 [==============================] - 2s 60ms/step - loss: 5.1535e-05 - accuracy: 1.0000\n",
      "Epoch 11/200\n",
      "24/24 [==============================] - 2s 59ms/step - loss: 4.3579e-05 - accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "24/24 [==============================] - 1s 62ms/step - loss: 3.6520e-05 - accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 3.7681e-05 - accuracy: 1.0000\n",
      "Epoch 14/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 3.4790e-05 - accuracy: 1.0000\n",
      "Epoch 15/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 3.2341e-05 - accuracy: 1.0000\n",
      "Epoch 16/200\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 2.9248e-05 - accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 2.6871e-05 - accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 2.6326e-05 - accuracy: 1.0000\n",
      "Epoch 19/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 2.4693e-05 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 2.3400e-05 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 2.0695e-05 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.8685e-05 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.9351e-05 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.8778e-05 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 1.7995e-05 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.7022e-05 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.6486e-05 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.5795e-05 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 1.5209e-05 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.4043e-05 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.2589e-05 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.3161e-05 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.3028e-05 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.2592e-05 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.0723e-05 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.1549e-05 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "24/24 [==============================] - 1s 64ms/step - loss: 1.1447e-05 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.1079e-05 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.0099e-05 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "24/24 [==============================] - 2s 59ms/step - loss: 9.7924e-06 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 9.9364e-06 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 9.6547e-06 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 9.2676e-06 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 9.1921e-06 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "24/24 [==============================] - 1s 59ms/step - loss: 9.0135e-06 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 7.9988e-06 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 8.3028e-06 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 8.2692e-06 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 7.5502e-06 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 7.8535e-06 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "24/24 [==============================] - 1s 64ms/step - loss: 7.5715e-06 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 7.0521e-06 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 7.1394e-06 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 6.7866e-06 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 6.4459e-06 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 5.6469e-06 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 5.9619e-06 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "24/24 [==============================] - 2s 59ms/step - loss: 5.2587e-06 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 6.3679e-06 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 6.0227e-06 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 5.9395e-06 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "24/24 [==============================] - 2s 68ms/step - loss: 5.6840e-06 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "24/24 [==============================] - 2s 68ms/step - loss: 5.7063e-06 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 5.5534e-06 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 5.3999e-06 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 5.0341e-06 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 5.0955e-06 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "24/24 [==============================] - 2s 71ms/step - loss: 5.1378e-06 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 4.8441e-06 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "24/24 [==============================] - 2s 60ms/step - loss: 4.5085e-06 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "24/24 [==============================] - 2s 60ms/step - loss: 4.4920e-06 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 4.5542e-06 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "24/24 [==============================] - 2s 59ms/step - loss: 4.6483e-06 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "24/24 [==============================] - 1s 59ms/step - loss: 4.3120e-06 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 4.4501e-06 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 4.3938e-06 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 4.0530e-06 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 4.1141e-06 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 3.9970e-06 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 4.0705e-06 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 3.9678e-06 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 3.7861e-06 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 3.7253e-06 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 3.5161e-06 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "24/24 [==============================] - 1s 62ms/step - loss: 3.6226e-06 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 3.5628e-06 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 3.4518e-06 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 3.3618e-06 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 3.0550e-06 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 3.3790e-06 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 3.2852e-06 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "24/24 [==============================] - 1s 59ms/step - loss: 3.1176e-06 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 3.2127e-06 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 2.7016e-06 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 3.0097e-06 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "24/24 [==============================] - 2s 58ms/step - loss: 3.0544e-06 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "24/24 [==============================] - 2s 61ms/step - loss: 2.9094e-06 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "24/24 [==============================] - 1s 59ms/step - loss: 2.9142e-06 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "24/24 [==============================] - 2s 61ms/step - loss: 2.7868e-06 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "24/24 [==============================] - 1s 57ms/step - loss: 2.7167e-06 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 2.7868e-06 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 2.6545e-06 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 2.5659e-06 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "24/24 [==============================] - 1s 55ms/step - loss: 2.4673e-06 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 2.6325e-06 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 2.3543e-06 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 2.5147e-06 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 2.5047e-06 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 2.2959e-06 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "24/24 [==============================] - 1s 64ms/step - loss: 2.4017e-06 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "24/24 [==============================] - 2s 61ms/step - loss: 2.3020e-06 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "24/24 [==============================] - 1s 58ms/step - loss: 2.1028e-06 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 2.1681e-06 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 2.1220e-06 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 2.1918e-06 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 2.0887e-06 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 2.1067e-06 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 2.1426e-06 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 2.0489e-06 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 2.0300e-06 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "24/24 [==============================] - 1s 62ms/step - loss: 2.0739e-06 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.9795e-06 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.9695e-06 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.9393e-06 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "24/24 [==============================] - 2s 68ms/step - loss: 1.9427e-06 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.8809e-06 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "24/24 [==============================] - 2s 61ms/step - loss: 1.9005e-06 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.8726e-06 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "24/24 [==============================] - 2s 67ms/step - loss: 1.7845e-06 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 1.8177e-06 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 1.6359e-06 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.5803e-06 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.6778e-06 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.6761e-06 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.6215e-06 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.6153e-06 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.6473e-06 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 1.6091e-06 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.5164e-06 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.4999e-06 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "24/24 [==============================] - 2s 61ms/step - loss: 1.4851e-06 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "24/24 [==============================] - 2s 54ms/step - loss: 1.5349e-06 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 1.4556e-06 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 1.4360e-06 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "24/24 [==============================] - 2s 67ms/step - loss: 1.4281e-06 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.4690e-06 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.3535e-06 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.3669e-06 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 1.4178e-06 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.3500e-06 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "24/24 [==============================] - 2s 61ms/step - loss: 1.3051e-06 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.2879e-06 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 1.2898e-06 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.3006e-06 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.3123e-06 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.1893e-06 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 1.2340e-06 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 1.2481e-06 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "24/24 [==============================] - 2s 59ms/step - loss: 1.2268e-06 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.1509e-06 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 1.2137e-06 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 1.1395e-06 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 1.1660e-06 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.1567e-06 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 1.0791e-06 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 1.1050e-06 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "24/24 [==============================] - 2s 62ms/step - loss: 1.0997e-06 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 1.0928e-06 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 1.0666e-06 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "24/24 [==============================] - 2s 71ms/step - loss: 1.0571e-06 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 1.0358e-06 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 9.9455e-07 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 9.9730e-07 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "24/24 [==============================] - 2s 70ms/step - loss: 1.0176e-06 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "24/24 [==============================] - 2s 67ms/step - loss: 9.9008e-07 - accuracy: 1.0000 0s - loss: 6.5\n",
      "Epoch 176/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 9.9489e-07 - accuracy: 1.0000 1s - los\n",
      "Epoch 177/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 9.1279e-07 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 9.0592e-07 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 9.8424e-07 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "24/24 [==============================] - 1s 62ms/step - loss: 9.3678e-07 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 8.7672e-07 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "24/24 [==============================] - 1s 59ms/step - loss: 8.7809e-07 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "24/24 [==============================] - 2s 61ms/step - loss: 9.3581e-07 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "24/24 [==============================] - 1s 59ms/step - loss: 8.5954e-07 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 8.9802e-07 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 8.6194e-07 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "24/24 [==============================] - 1s 56ms/step - loss: 8.4202e-07 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 8.4614e-07 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 8.7397e-07 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 8.4477e-07 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 8.5233e-07 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 8.4442e-07 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 8.1316e-07 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "24/24 [==============================] - 1s 57ms/step - loss: 8.0492e-07 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "24/24 [==============================] - 1s 60ms/step - loss: 7.9221e-07 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "24/24 [==============================] - 2s 63ms/step - loss: 7.7606e-07 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "24/24 [==============================] - 1s 61ms/step - loss: 7.2693e-07 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "24/24 [==============================] - 1s 58ms/step - loss: 7.8018e-07 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 7.6988e-07 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 7.4205e-07 - accuracy: 1.0000\n",
      "Trining Complete!\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "\n",
    "model.fit(\n",
    "    train_batches,\n",
    "    validation_data=valid_batches,\n",
    "    steps_per_epoch=train_batches.samples // BATCH_SIZE_TRAIN,\n",
    "    validation_steps=valid_batches.samples // BATCH_SIZE_VALIDATE,\n",
    "    epochs=200,  # Was 30\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        ReduceLROnPlateau(),\n",
    "        # EarlyStopping(\n",
    "        #     monitor='loss',\n",
    "        #     mode='min',\n",
    "        #     verbose=1,\n",
    "        #     baseline=3.0,\n",
    "        # )\n",
    "    ]\n",
    ")\n",
    "print(\"Trining Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amJJCzkTOpcF"
   },
   "source": [
    "## Testing the Model\n",
    "\n",
    "To perform a final test of the model, the test data is fed through the model.  It is important to use data that was not used in training so we can test the performance accurately. The accuracy is calculated and a confusion matrix is generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FK3o1TFt5H9"
   },
   "source": [
    "### Test Accuracy\n",
    "\n",
    "To calculate accuracy, the model is evaluated using the test generator specified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "t5syFibUt_EA",
    "outputId": "78974f30-434e-47d3-b326-ec27113f4eec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44212836027145386\n",
      "0.9624060392379761\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_batches,\n",
    "                                     steps=len(test_batches),\n",
    "                                     verbose=0)\n",
    "print(test_loss)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIOd7ID2uQlc"
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix gives more insight than just accuracy. It allows us to determine true positives, false positives, false negatives, and true negatives. The code below generates a confusion matrix for our test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "9yqGOy1KOs5q",
    "outputId": "87d64ba3-ec79-428e-e241-cb015b010a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 133 images belonging to 3 classes.\n",
      "1/1 [==============================] - 1s 696ms/step\n",
      "\n",
      "(133, 3)\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "(133, 3)\n",
      "[[9.99988675e-01 2.55536614e-09 1.13217784e-05]\n",
      " [1.00000000e+00 5.36402249e-15 4.31359943e-08]\n",
      " [1.00000000e+00 4.17439700e-16 7.29887338e-16]\n",
      " [1.00000000e+00 8.85451312e-15 3.59872112e-15]\n",
      " [9.06356215e-01 9.33591276e-02 2.84546841e-04]\n",
      " [9.99997377e-01 2.63090055e-06 5.25758437e-10]\n",
      " [1.00000000e+00 5.87728104e-17 7.10067466e-15]\n",
      " [1.00000000e+00 2.47707332e-15 6.23850743e-13]\n",
      " [1.00000000e+00 4.31819869e-09 1.33197442e-09]\n",
      " [1.00000000e+00 1.85957867e-15 3.00071242e-16]\n",
      " [1.00000000e+00 1.01336712e-12 5.10754494e-10]\n",
      " [1.00000000e+00 1.57921717e-13 7.48838769e-11]\n",
      " [1.00000000e+00 1.50885800e-17 1.16226039e-17]\n",
      " [1.00000000e+00 1.74756959e-18 4.39134553e-18]\n",
      " [1.00000000e+00 2.98182079e-13 1.94166780e-08]\n",
      " [1.00000000e+00 5.14580622e-18 9.68777080e-19]\n",
      " [1.00000000e+00 4.49612101e-24 6.18299351e-18]\n",
      " [9.99997497e-01 2.54111046e-06 5.48768808e-10]\n",
      " [1.54178347e-02 9.81952846e-01 2.62935087e-03]\n",
      " [3.11409704e-10 9.99999881e-01 7.63251151e-08]\n",
      " [4.86165372e-05 9.99718964e-01 2.32415434e-04]\n",
      " [2.24941403e-01 7.75058568e-01 2.03011746e-10]\n",
      " [2.82383644e-06 9.99801934e-01 1.95174202e-04]\n",
      " [3.17863208e-10 1.00000000e+00 2.29444779e-11]\n",
      " [9.74685471e-11 9.99999404e-01 5.60589854e-07]\n",
      " [8.96168295e-09 1.00000000e+00 1.22178641e-14]\n",
      " [6.06322132e-17 1.00000000e+00 1.88572751e-11]\n",
      " [1.21762533e-09 1.00000000e+00 5.34199039e-11]\n",
      " [1.19403076e-05 9.99751747e-01 2.36303036e-04]\n",
      " [6.55704495e-13 1.00000000e+00 2.51596327e-10]\n",
      " [1.77503330e-13 1.00000000e+00 3.32678622e-16]\n",
      " [1.12824654e-11 1.00000000e+00 5.09868276e-12]\n",
      " [2.88502945e-13 1.00000000e+00 5.62666413e-10]\n",
      " [4.11297176e-16 9.99999881e-01 1.14085822e-07]\n",
      " [3.30335332e-16 1.00000000e+00 7.63315870e-14]\n",
      " [1.67070970e-15 1.00000000e+00 1.01713651e-10]\n",
      " [3.08448178e-09 7.54724205e-01 2.45275810e-01]\n",
      " [8.78806588e-07 9.69154835e-01 3.08441706e-02]\n",
      " [3.52148642e-25 1.00000000e+00 6.15748822e-15]\n",
      " [5.44381929e-30 1.00000000e+00 1.45894163e-17]\n",
      " [1.49853911e-24 1.00000000e+00 1.24450775e-12]\n",
      " [6.03235943e-15 1.00000000e+00 3.13278570e-10]\n",
      " [6.25594177e-20 1.00000000e+00 6.33624479e-16]\n",
      " [7.90243938e-18 1.00000000e+00 5.37515507e-19]\n",
      " [2.88217042e-16 1.00000000e+00 4.91708682e-15]\n",
      " [1.92829531e-13 1.00000000e+00 7.48813025e-11]\n",
      " [2.10900078e-18 1.00000000e+00 3.73547300e-16]\n",
      " [5.68987480e-06 9.99994278e-01 4.50836607e-13]\n",
      " [1.59599727e-15 1.00000000e+00 2.87448641e-11]\n",
      " [1.45954210e-18 1.00000000e+00 8.32679842e-17]\n",
      " [1.92240290e-15 1.00000000e+00 6.94994826e-11]\n",
      " [4.94025514e-08 6.37375534e-01 3.62624377e-01]\n",
      " [7.87206248e-15 1.00000000e+00 5.78387923e-13]\n",
      " [3.32251051e-03 9.89573896e-01 7.10363733e-03]\n",
      " [7.63832805e-18 1.00000000e+00 4.02331036e-15]\n",
      " [6.51244267e-11 1.00000000e+00 2.39272594e-14]\n",
      " [2.62542158e-12 9.99999523e-01 4.27498037e-07]\n",
      " [3.61854502e-04 9.97247040e-01 2.39109993e-03]\n",
      " [4.67382451e-21 1.00000000e+00 1.13399557e-12]\n",
      " [1.88684344e-18 1.00000000e+00 9.39340943e-17]\n",
      " [1.70146048e-12 9.99998927e-01 1.07663743e-06]\n",
      " [1.78683767e-04 9.99616027e-01 2.05265387e-04]\n",
      " [1.47012330e-21 1.00000000e+00 1.31795131e-20]\n",
      " [3.83358428e-10 1.00000000e+00 1.41467402e-11]\n",
      " [9.48297173e-20 1.00000000e+00 8.77611416e-18]\n",
      " [2.74077500e-13 1.00000000e+00 1.27005238e-14]\n",
      " [1.45298617e-19 1.00000000e+00 3.63293041e-14]\n",
      " [9.61763655e-13 1.00000000e+00 8.63076988e-09]\n",
      " [5.16778393e-14 9.99998808e-01 1.17141485e-06]\n",
      " [2.85103243e-20 1.00000000e+00 2.15925997e-12]\n",
      " [1.50467610e-13 3.47562850e-01 6.52437210e-01]\n",
      " [6.97621488e-07 9.99961257e-01 3.79903831e-05]\n",
      " [1.19268639e-08 1.00000000e+00 9.02364905e-10]\n",
      " [2.09995667e-15 1.00000000e+00 9.94183812e-17]\n",
      " [2.31307837e-11 1.00000000e+00 1.73003462e-15]\n",
      " [1.89535453e-12 1.00000000e+00 1.76649591e-12]\n",
      " [2.56761368e-08 9.98541236e-01 1.45882845e-03]\n",
      " [1.19437188e-01 9.38331783e-02 7.86729574e-01]\n",
      " [3.60060238e-15 4.82638263e-10 1.00000000e+00]\n",
      " [2.66610677e-19 8.74960116e-19 1.00000000e+00]\n",
      " [2.42562029e-19 1.28310868e-13 1.00000000e+00]\n",
      " [5.21810410e-22 4.21204835e-17 1.00000000e+00]\n",
      " [2.04822612e-19 1.00000000e+00 4.33209051e-13]\n",
      " [8.16073470e-16 1.91768912e-08 1.00000000e+00]\n",
      " [4.88664700e-29 1.33414782e-21 1.00000000e+00]\n",
      " [4.22208140e-27 2.86114310e-20 1.00000000e+00]\n",
      " [1.19162067e-18 8.52533308e-14 1.00000000e+00]\n",
      " [3.24102792e-14 8.22741143e-15 1.00000000e+00]\n",
      " [4.09192697e-16 1.62498448e-02 9.83750165e-01]\n",
      " [2.08307628e-13 9.19529990e-12 1.00000000e+00]\n",
      " [2.50713970e-03 9.92102828e-03 9.87571836e-01]\n",
      " [7.89959424e-21 1.12274929e-14 1.00000000e+00]\n",
      " [1.14333503e-07 1.06790561e-04 9.99893069e-01]\n",
      " [6.54245177e-21 6.79694601e-15 1.00000000e+00]\n",
      " [8.40653840e-14 6.08262218e-10 1.00000000e+00]\n",
      " [4.18934017e-18 3.14315861e-17 1.00000000e+00]\n",
      " [1.57909699e-12 3.51561482e-07 9.99999642e-01]\n",
      " [1.23878522e-12 6.80011340e-07 9.99999285e-01]\n",
      " [2.41799548e-14 2.01641811e-11 1.00000000e+00]\n",
      " [1.77382117e-21 7.77787590e-18 1.00000000e+00]\n",
      " [1.55334503e-15 2.16034297e-16 1.00000000e+00]\n",
      " [2.47053017e-10 2.79990587e-07 9.99999762e-01]\n",
      " [6.32379077e-14 5.56151118e-15 1.00000000e+00]\n",
      " [1.70677297e-20 3.81894649e-09 1.00000000e+00]\n",
      " [4.66942601e-27 3.73701702e-21 1.00000000e+00]\n",
      " [1.48755327e-23 2.39918807e-10 1.00000000e+00]\n",
      " [4.41839397e-20 5.69984683e-17 1.00000000e+00]\n",
      " [2.42679270e-29 1.58295015e-21 1.00000000e+00]\n",
      " [1.44118728e-22 5.21103993e-12 1.00000000e+00]\n",
      " [3.06226683e-16 8.83379148e-09 1.00000000e+00]\n",
      " [2.39795427e-20 1.30985098e-10 1.00000000e+00]\n",
      " [6.76967107e-23 5.98774155e-16 1.00000000e+00]\n",
      " [1.10363714e-20 1.00881675e-14 1.00000000e+00]\n",
      " [9.59085513e-14 3.75066738e-04 9.99624968e-01]\n",
      " [8.74831535e-23 1.37321202e-14 1.00000000e+00]\n",
      " [2.97567869e-17 2.25094380e-04 9.99774873e-01]\n",
      " [8.11662737e-21 5.07905340e-17 1.00000000e+00]\n",
      " [1.14229816e-21 3.21810395e-12 1.00000000e+00]\n",
      " [2.28214313e-18 3.61072132e-04 9.99638915e-01]\n",
      " [8.22003219e-13 2.63236264e-12 1.00000000e+00]\n",
      " [4.05883067e-21 1.31753243e-15 1.00000000e+00]\n",
      " [4.12557304e-13 9.86920008e-12 1.00000000e+00]\n",
      " [2.02123961e-18 3.17375813e-12 1.00000000e+00]\n",
      " [8.68283310e-14 9.99867439e-01 1.32532427e-04]\n",
      " [1.48041550e-22 2.88343873e-21 1.00000000e+00]\n",
      " [1.28632563e-21 2.70320130e-14 1.00000000e+00]\n",
      " [1.27373153e-06 1.39199302e-01 8.60799372e-01]\n",
      " [2.34392532e-20 1.19286473e-14 1.00000000e+00]\n",
      " [2.80265172e-10 3.14451754e-04 9.99685526e-01]\n",
      " [1.11296238e-07 6.00713967e-09 9.99999881e-01]\n",
      " [5.03724300e-16 3.33334818e-08 1.00000000e+00]\n",
      " [6.77667783e-17 2.12390393e-01 7.87609637e-01]\n",
      " [2.06942919e-21 1.23268219e-11 1.00000000e+00]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2]\n",
      "\n",
      "3\n",
      "[9.9998868e-01 2.5553661e-09 1.1321778e-05]\n",
      "Confusion matrix, without normalization\n",
      "[[18  0  0]\n",
      " [ 0 58  3]\n",
      " [ 0  2 52]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # thresh = cm.max() / 2.\n",
    "    # for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    #     plt.text(j, i, cm[i, j],\n",
    "    #              horizontalalignment=\"center\",\n",
    "    #              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    thresh = cm.max() / 3.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(title + \"_cat_dog_wolves\")\n",
    "\n",
    "\n",
    "# test_batches.reset()\n",
    "\n",
    "# * Should this be random?\n",
    "test_batches = ImageDataGenerator().flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(224, 224),\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    # seed=SEED,\n",
    ")\n",
    "\n",
    "test_imgs, test_labels = next(test_batches)\n",
    "# test_labels = test_labels[:,0]\n",
    "\n",
    "predictions = model.predict(test_batches, \n",
    "                            # steps=len(test_batches),\n",
    "                            verbose=1)\n",
    "# print(test_imgs)\n",
    "print()\n",
    "print(test_labels.shape)\n",
    "print(test_labels)\n",
    "print(test_labels.argmax(axis=1))\n",
    "print()\n",
    "print(predictions.shape)\n",
    "print(predictions)\n",
    "print(predictions.argmax(axis=1))\n",
    "print()\n",
    "print(len(predictions[0]))\n",
    "print(predictions[0])\n",
    "\n",
    "\n",
    "# cm = confusion_matrix(test_labels, np.round(predictions[:,0]))\n",
    "cm = confusion_matrix(test_labels.argmax(axis=1),\n",
    "                      predictions.argmax(axis=1),\n",
    "                      # binary=False\n",
    "                      )\n",
    "\n",
    "cm_plot_labels = classes\n",
    "plot_confusion_matrix(cm,\n",
    "                      cm_plot_labels,\n",
    "                      title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 51.,  47.,  36.],\n",
       "         [ 52.,  48.,  37.],\n",
       "         [ 52.,  48.,  37.],\n",
       "         ...,\n",
       "         [250., 248., 251.],\n",
       "         [226., 224., 227.],\n",
       "         [209., 207., 210.]],\n",
       "\n",
       "        [[ 51.,  47.,  36.],\n",
       "         [ 52.,  48.,  37.],\n",
       "         [ 52.,  48.,  37.],\n",
       "         ...,\n",
       "         [250., 248., 251.],\n",
       "         [226., 224., 227.],\n",
       "         [209., 207., 210.]],\n",
       "\n",
       "        [[ 51.,  47.,  36.],\n",
       "         [ 51.,  47.,  36.],\n",
       "         [ 52.,  48.,  37.],\n",
       "         ...,\n",
       "         [250., 248., 251.],\n",
       "         [226., 224., 227.],\n",
       "         [213., 211., 214.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[134., 135., 139.],\n",
       "         [131., 132., 136.],\n",
       "         [133., 134., 138.],\n",
       "         ...,\n",
       "         [181., 178., 187.],\n",
       "         [179., 176., 185.],\n",
       "         [179., 176., 185.]],\n",
       "\n",
       "        [[128., 129., 134.],\n",
       "         [125., 126., 131.],\n",
       "         [127., 128., 133.],\n",
       "         ...,\n",
       "         [181., 178., 187.],\n",
       "         [179., 176., 185.],\n",
       "         [179., 176., 185.]],\n",
       "\n",
       "        [[128., 129., 134.],\n",
       "         [125., 126., 131.],\n",
       "         [127., 128., 133.],\n",
       "         ...,\n",
       "         [181., 178., 187.],\n",
       "         [179., 176., 185.],\n",
       "         [179., 176., 185.]]],\n",
       "\n",
       "\n",
       "       [[[129., 104.,  50.],\n",
       "         [115.,  89.,  40.],\n",
       "         [108.,  81.,  34.],\n",
       "         ...,\n",
       "         [ 58.,  32.,   9.],\n",
       "         [ 58.,  32.,   9.],\n",
       "         [ 61.,  35.,  12.]],\n",
       "\n",
       "        [[131., 106.,  52.],\n",
       "         [113.,  87.,  38.],\n",
       "         [105.,  78.,  31.],\n",
       "         ...,\n",
       "         [ 59.,  33.,  10.],\n",
       "         [ 59.,  33.,  10.],\n",
       "         [ 60.,  34.,  11.]],\n",
       "\n",
       "        [[132., 106.,  55.],\n",
       "         [112.,  86.,  37.],\n",
       "         [103.,  76.,  31.],\n",
       "         ...,\n",
       "         [ 61.,  35.,  12.],\n",
       "         [ 60.,  34.,  11.],\n",
       "         [ 58.,  32.,   9.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[206., 183., 129.],\n",
       "         [201., 178., 124.],\n",
       "         [200., 177., 123.],\n",
       "         ...,\n",
       "         [170., 145.,  78.],\n",
       "         [170., 145.,  78.],\n",
       "         [175., 150.,  83.]],\n",
       "\n",
       "        [[206., 183., 129.],\n",
       "         [201., 178., 124.],\n",
       "         [201., 178., 124.],\n",
       "         ...,\n",
       "         [172., 147.,  80.],\n",
       "         [171., 146.,  79.],\n",
       "         [175., 150.,  83.]],\n",
       "\n",
       "        [[205., 182., 128.],\n",
       "         [201., 178., 124.],\n",
       "         [201., 178., 124.],\n",
       "         ...,\n",
       "         [175., 150.,  83.],\n",
       "         [174., 149.,  82.],\n",
       "         [176., 151.,  84.]]],\n",
       "\n",
       "\n",
       "       [[[152., 137., 130.],\n",
       "         [162., 147., 140.],\n",
       "         [155., 140., 133.],\n",
       "         ...,\n",
       "         [102., 100., 103.],\n",
       "         [100., 100., 102.],\n",
       "         [ 96.,  96.,  98.]],\n",
       "\n",
       "        [[152., 137., 130.],\n",
       "         [160., 145., 138.],\n",
       "         [158., 143., 136.],\n",
       "         ...,\n",
       "         [103., 101., 104.],\n",
       "         [ 98.,  98., 100.],\n",
       "         [ 92.,  92.,  94.]],\n",
       "\n",
       "        [[155., 140., 133.],\n",
       "         [157., 142., 135.],\n",
       "         [161., 146., 139.],\n",
       "         ...,\n",
       "         [104., 102., 105.],\n",
       "         [ 99.,  99., 101.],\n",
       "         [ 95.,  95.,  97.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 19.,  35.,  48.],\n",
       "         [ 19.,  35.,  48.],\n",
       "         [ 18.,  34.,  47.],\n",
       "         ...,\n",
       "         [ 23.,  50.,  67.],\n",
       "         [ 23.,  50.,  67.],\n",
       "         [ 32.,  59.,  76.]],\n",
       "\n",
       "        [[ 18.,  34.,  47.],\n",
       "         [ 18.,  34.,  47.],\n",
       "         [ 21.,  37.,  50.],\n",
       "         ...,\n",
       "         [ 18.,  45.,  62.],\n",
       "         [ 21.,  48.,  65.],\n",
       "         [ 29.,  56.,  73.]],\n",
       "\n",
       "        [[ 20.,  36.,  49.],\n",
       "         [ 19.,  35.,  48.],\n",
       "         [ 22.,  38.,  51.],\n",
       "         ...,\n",
       "         [ 17.,  44.,  61.],\n",
       "         [ 15.,  42.,  59.],\n",
       "         [ 23.,  50.,  67.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[  0.,   0.,   0.],\n",
       "         [  2.,  12.,   4.],\n",
       "         [  0.,  11.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   7.],\n",
       "         [  0.,   9.,   0.],\n",
       "         [ 29.,  55.,  18.]],\n",
       "\n",
       "        [[  0.,   2.,   0.],\n",
       "         [  5.,  15.,   7.],\n",
       "         [  0.,   8.,   0.],\n",
       "         ...,\n",
       "         [  0.,   1.,   5.],\n",
       "         [  0.,  13.,   0.],\n",
       "         [ 46.,  71.,  32.]],\n",
       "\n",
       "        [[  0.,   9.,   0.],\n",
       "         [  4.,  16.,   6.],\n",
       "         [  0.,   5.,   0.],\n",
       "         ...,\n",
       "         [  0.,   3.,   2.],\n",
       "         [ 31.,  44.,  14.],\n",
       "         [ 31.,  53.,  14.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 98.,  96.,  57.],\n",
       "         [ 89.,  94.,  53.],\n",
       "         [ 88.,  95.,  54.],\n",
       "         ...,\n",
       "         [ 61.,  77.,  40.],\n",
       "         [ 59.,  71.,  33.],\n",
       "         [ 62.,  68.,  30.]],\n",
       "\n",
       "        [[116., 108.,  71.],\n",
       "         [ 94.,  95.,  55.],\n",
       "         [ 81.,  88.,  47.],\n",
       "         ...,\n",
       "         [ 61.,  79.,  41.],\n",
       "         [ 65.,  77.,  39.],\n",
       "         [ 58.,  64.,  26.]],\n",
       "\n",
       "        [[126., 112.,  77.],\n",
       "         [ 97.,  93.,  55.],\n",
       "         [ 86.,  93.,  52.],\n",
       "         ...,\n",
       "         [ 57.,  75.,  37.],\n",
       "         [ 66.,  81.,  42.],\n",
       "         [ 78.,  82.,  45.]]],\n",
       "\n",
       "\n",
       "       [[[255., 252., 255.],\n",
       "         [255., 255., 225.],\n",
       "         [255., 251., 252.],\n",
       "         ...,\n",
       "         [255., 252., 255.],\n",
       "         [255., 255., 228.],\n",
       "         [254., 252., 255.]],\n",
       "\n",
       "        [[243., 242., 247.],\n",
       "         [ 33.,  36.,   7.],\n",
       "         [ 60.,  58.,  46.],\n",
       "         ...,\n",
       "         [ 18.,  13.,   9.],\n",
       "         [  3.,   3.,   0.],\n",
       "         [252., 249., 255.]],\n",
       "\n",
       "        [[252., 254., 241.],\n",
       "         [ 77.,  78.,  60.],\n",
       "         [ 58.,  57.,  36.],\n",
       "         ...,\n",
       "         [ 35.,  27.,  14.],\n",
       "         [ 69.,  70.,  36.],\n",
       "         [255., 252., 255.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[253., 251., 252.],\n",
       "         [ 84.,  81.,  88.],\n",
       "         [102.,  95., 103.],\n",
       "         ...,\n",
       "         [212., 190., 176.],\n",
       "         [222., 199., 183.],\n",
       "         [255., 250., 245.]],\n",
       "\n",
       "        [[239., 242., 235.],\n",
       "         [ 13.,  13.,  15.],\n",
       "         [ 62.,  57.,  64.],\n",
       "         ...,\n",
       "         [175., 161., 152.],\n",
       "         [183., 166., 156.],\n",
       "         [245., 243., 244.]],\n",
       "\n",
       "        [[252., 255., 246.],\n",
       "         [254., 255., 253.],\n",
       "         [255., 253., 255.],\n",
       "         ...,\n",
       "         [255., 254., 251.],\n",
       "         [255., 252., 250.],\n",
       "         [249., 255., 255.]]],\n",
       "\n",
       "\n",
       "       [[[ 29.,  40.,  24.],\n",
       "         [ 24.,  35.,  19.],\n",
       "         [ 23.,  36.,  19.],\n",
       "         ...,\n",
       "         [ 23.,  37.,  14.],\n",
       "         [ 22.,  37.,  14.],\n",
       "         [ 21.,  39.,  15.]],\n",
       "\n",
       "        [[ 30.,  41.,  25.],\n",
       "         [ 26.,  37.,  21.],\n",
       "         [ 36.,  49.,  32.],\n",
       "         ...,\n",
       "         [ 21.,  34.,  14.],\n",
       "         [ 25.,  40.,  19.],\n",
       "         [ 24.,  42.,  20.]],\n",
       "\n",
       "        [[ 28.,  39.,  23.],\n",
       "         [ 32.,  45.,  28.],\n",
       "         [ 33.,  47.,  30.],\n",
       "         ...,\n",
       "         [ 22.,  35.,  17.],\n",
       "         [ 21.,  36.,  17.],\n",
       "         [ 26.,  41.,  22.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 19.,  26.,  19.],\n",
       "         [ 33.,  40.,  33.],\n",
       "         [ 33.,  42.,  37.],\n",
       "         ...,\n",
       "         [  9.,   9.,   9.],\n",
       "         [  8.,   8.,   8.],\n",
       "         [  8.,   8.,   8.]],\n",
       "\n",
       "        [[ 19.,  24.,  18.],\n",
       "         [ 32.,  39.,  32.],\n",
       "         [ 34.,  43.,  38.],\n",
       "         ...,\n",
       "         [  6.,   6.,   6.],\n",
       "         [  7.,   7.,   7.],\n",
       "         [  5.,   5.,   5.]],\n",
       "\n",
       "        [[ 20.,  25.,  19.],\n",
       "         [ 34.,  39.,  33.],\n",
       "         [ 37.,  46.,  41.],\n",
       "         ...,\n",
       "         [  1.,   1.,   1.],\n",
       "         [  1.,   1.,   1.],\n",
       "         [  1.,   1.,   1.]]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_imgs)\n",
    "print(\"#\" * 25)\n",
    "display(test_labels)\n",
    "print(\"#\" * 25)\n",
    "display(test_labels.argmax(axis=1))\n",
    "print(\"#\" * 25)\n",
    "display(predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBgD03P9O1oq"
   },
   "source": [
    "## Saving the fine-tuned VGG16 model\n",
    "\n",
    "Now that the model has been trained and tested, it should be saved so we can classify new images without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(path_abs_model, model_given, test_batches_given, steps, optimzier=Adam(learning_rate=.0001), loss='categorical_crossentropy', metricts=['accuracy']):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "\n",
    "    model_given_test_loss, model_given_test_acc = model_given.evaluate(\n",
    "        test_batches_given,\n",
    "        steps=steps,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save if there is no model to compare to\n",
    "    if not os.path.exists(path_abs_model):\n",
    "        print(f\"Current model with acc ({model_given_test_acc}) is saved!\")\n",
    "        model_given.save(path_abs_model)\n",
    "        return\n",
    "\n",
    "    model_existing = tf.keras.models.clone_model(model_given)\n",
    "    model_existing.load_weights(path_abs_model)\n",
    "\n",
    "    model_existing.compile(optimzier,\n",
    "                           loss=loss,\n",
    "                           metrics=metricts)\n",
    "\n",
    "    model_existing_test_loss, model_existing_test_acc = model_existing.evaluate(\n",
    "        test_batches_given,\n",
    "        steps=steps,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save model given if it's better than the existing one\n",
    "    if model_given_test_acc >= model_existing_test_acc:\n",
    "        print(\n",
    "            f\"Current model with acc ({model_given_test_acc}) is better than Existing model with acc ({model_existing_test_acc}) and is saved!\")\n",
    "        model_existing.save(path_abs_model)\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            f\"Existing model with acc ({model_existing_test_acc}) is better than Current model with acc ({model_given_test_acc})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t9xDc240_nCV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 925ms/step - loss: 0.4421 - accuracy: 0.9624\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4800 - accuracy: 0.9624\n",
      "Current model with acc (0.9624060392379761) is better than Existing model with acc (0.9624060392379761) and is saved!\n"
     ]
    }
   ],
   "source": [
    "save_best_model('weights_animals_VGG16_fine_tuned.h5', model, test_batches, len(test_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNkDKsfRcVS9"
   },
   "source": [
    "## Loading the fine-tuned VGG16 model\n",
    "\n",
    "In a production environment, the model could be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j4acEqPacdvM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded.\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights_animals_VGG16_fine_tuned.h5')\n",
    "print(\"Model Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNHYAMiphMX_"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, using transfer learning for image classification has been demonstrated. First, the data is uploaded and connected to the Keras API. Then, the model is defined by copying a pretrained model and adding a new output layer. Once defined, the model can be trained using the appropriate loss function. Finally, the model is evaluated using a test dataset. With minimal modification, the model can be used to predict multiple classes beyond two, as well as predicting scalar values. It is encouraged to use this notebook as a templete for similar models using novel datasets. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cats-Dogs-Wolves Transfer Learning Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit Anaconda (Tensorflow 2.5.0 | AutoKeras | CUDE: 11.2 | GPU Support: True)",
   "language": "python",
   "name": "env_conda_38_tensorflow_gpu_autokeras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
