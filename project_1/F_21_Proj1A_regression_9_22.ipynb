{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "F_21_Proj1A_regression9.22.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhGuhbZ6M5tl"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AwOEIRJC6Une"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9r1o7HsyNTf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KyPEtTqk6VdG"
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 Fran√ßois Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdT9iu_Z4Rb"
      },
      "source": [
        "# Proj1A - Basic Regression: Understanding the ADNI Data Using Regression  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBIlTPscrIT9"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/regression\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/regression.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/regression.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/regression.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHp3M9ZmrIxj"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "Please make a copy and rename it with your name (ex: Proj1A_Ilmi_Yoon). All grading points should be explored in the notebook but some can be done in a separate pdf file. \n",
        "\n",
        "*Graded questions will be listed with \"Q:\" followed by the corresponding points.* \n",
        "\n",
        "You will be submitting **a pdf** file containing **the url of your own proj1A.**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "nXcCkWJdz43N"
      },
      "source": [
        ">[Proj1A - Basic Regression: Understanding the ADNI Data Using Regression](#scrollTo=EIdT9iu_Z4Rb)\n",
        "\n",
        ">>[1. Load in the Data](#scrollTo=gFh9ne3FZ-On)\n",
        "\n",
        ">>[2. Clean the Data](#scrollTo=3MWuJTKEDM-f)\n",
        "\n",
        ">>[3Inspect the Data](#scrollTo=J4ubs136WLNp)\n",
        "\n",
        ">>[Select a Few Features to Work On and Split Features from Labels](#scrollTo=Db7Auq1yXUvh)\n",
        "\n",
        ">>[Normalization](#scrollTo=mRklxK5s388r)\n",
        "\n",
        ">>>[5.1 The Normalization Layer](#scrollTo=aFJ6ISropeoo)\n",
        "\n",
        ">>[Linear regression](#scrollTo=6o3CrycBXA2s)\n",
        "\n",
        ">>>[6.1. One Variable](#scrollTo=lFby9n0tnHkw)\n",
        "\n",
        ">>>[6.2. Multiple Variables (Features)](#scrollTo=Yk2RmlqPoM9u)\n",
        "\n",
        ">[Extra Credit: A DNN regression](#scrollTo=SmjdzxKzEu1-)\n",
        "\n",
        ">>[Instructions](#scrollTo=DT_aHPsrzO1t)\n",
        "\n",
        ">>[A. Train the Model](#scrollTo=ELz48lsgqC46)\n",
        "\n",
        ">>>[A1. One Variable](#scrollTo=7T4RP1V36gVn)\n",
        "\n",
        ">>>[A2. Full Model](#scrollTo=S_2Btebp2e64)\n",
        "\n",
        ">>[B. Performance](#scrollTo=uiCucdPLfMkZ)\n",
        "\n",
        ">>[C. Make Predictions](#scrollTo=ft603OzXuEZC)\n",
        "\n",
        ">[Conclusion](#scrollTo=vgGQuV-yqYZH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F-jbPqgsvIc"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "Introduction (Points: 30 points)\n",
        "1. Load in the Data\n",
        "2. Clean the Data\n",
        "3. Inspect the Data\n",
        "4. Select a Few Features to Work On and Split Features from Labels\n",
        "5. Normalization\n",
        "\n",
        "  5.1 The Normalization Layer\n",
        "\n",
        "6. Linear Regression\n",
        "\n",
        "  6.1 One Variable\n",
        " \n",
        "  6.2 Multiple Variable (Features)\n",
        "\n",
        "A DNN Regression (Extra Credit: 3 points)\n",
        "\n",
        "1. Instructions\n",
        "2. A. Train the Model \n",
        "\n",
        "  A1. One Variable \n",
        "  \n",
        "  A2. Full Model\n",
        "\n",
        "3. B. Performance\n",
        "4. C. Make Predictions\n",
        "\n",
        "Conclusion\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig9iJgBLsFfz"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In a **regression** problem, the aim is to *predict the output of a continuous value*, like a price or a probability.\n",
        "\n",
        "Contrast this with a **classification** problem, where the aim is to *select a class from a list of classes* (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\n",
        "\n",
        "This example uses the `tf.keras API`, see [this guide](https://www.tensorflow.org/guide/keras) for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moB4tpEHxKB3"
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install -q seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Make numpy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xQKvCJ85kCQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFh9ne3FZ-On"
      },
      "source": [
        "### 1. Load in the Data\n",
        "First download and import the dataset using pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/pleunipennings/CSC508Data/main/PatData.csv\" \n",
        "data = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oY3pMPagJrO"
      },
      "source": [
        "dataset = data.copy()\n",
        "dataset.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_fWRV8xhOUr"
      },
      "source": [
        "dataset['DX'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-I7yjN1hRdb"
      },
      "source": [
        "So, I would like to take out everything in the dataset **EXCEPT** the following levels: NL, MCI and Dementia. \n",
        "\n",
        "*NL = cognitively normal , MCI = mild cognitive impairement.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Nfsz_BhWaU"
      },
      "source": [
        "index_to_drop = dataset[ (dataset['DX'] != \"MCI\") & (dataset['DX'] != \"NL\") & (dataset['DX'] != \"Dementia\")].index\n",
        "  \n",
        "# drop these given row indices from data\n",
        "dataset = dataset.drop(index_to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKebSjP-hb2L"
      },
      "source": [
        "dataset['DX'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MWuJTKEDM-f"
      },
      "source": [
        "### 2. Clean the Data\n",
        "\n",
        "The dataset contains a few unknown values. To see how many unknown values, use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEJHhN65a2VV"
      },
      "source": [
        "dataset.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UPN0KBHa_WI"
      },
      "source": [
        "Drop those rows to keep this initial tutorial simple.\n",
        "\n",
        "Q: **(1 point)** What are other ways to process these rows instead of dropping? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZUDosChC1UN"
      },
      "source": [
        "dataset = dataset.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6SLJuXALT8y"
      },
      "source": [
        "Categorical Data needs to be properly handled using one-hot-encoding. \n",
        "\n",
        "Q: **(2 points)** Explain in 200 words what is one-hot-encoding and why it is necessary to handle categorical data.\n",
        "\n",
        "Q: **(1 point)** Make one more categorical feature into one-hot-encoding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA2Ccz-VMD-6"
      },
      "source": [
        "Q: **(1 point)** Explain why DX column is mapped to numeric values as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny2VXtnu5Ji-"
      },
      "source": [
        "cleanup_DX = {\"DX\": {\"NL\": 1, \"MCI\": 2, \"Dementia\": 3}}\n",
        "dataset = dataset.replace(cleanup_DX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn-IGhUE7_1H"
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4ubs136WLNp"
      },
      "source": [
        "### 3. Inspect the Data\n",
        "\n",
        "Have a quick look at the joint distribution of a few pairs of columns from the training set. *Can you find the data that show their relationship clearly?*\n",
        "\n",
        "Q: **(2 points)** Please work with different columns and write what you have learned from the visualization of the data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPKLN_f0gssF"
      },
      "source": [
        "sns.pairplot(train_dataset[[\"AGE\", \"Hippocampus\", \"Ventricles\", \"WholeBrain\", \"Entorhinal\", \"Fusiform\", \"MidTemp\", \"ICV\", \"APOE4\", \"DX\"]], diag_kind=\"kde\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gavKO_6DWRMP"
      },
      "source": [
        "Also look at the overall statistics, note how each feature covers a very different range:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi2FzC3T21jR"
      },
      "source": [
        "train_dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlOUYti2OIqY"
      },
      "source": [
        "The code below allows you to look into different groups of data -- normal patients, mild patients and dimential patients. \n",
        "\n",
        "Q: **(3 points)** Play with the total data and/or each group data and its regression on age, DX, and other columns "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb10GVpP7f4F"
      },
      "source": [
        "index_to_drop = train_dataset[ (train_dataset['DX'] != 1) ].index\n",
        "\n",
        "# drop these given row indices from data\n",
        "train_dataset = train_dataset.drop(index_to_drop)\n",
        "train_dataset.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db7Auq1yXUvh"
      },
      "source": [
        "### 4. Select a Few Features to Work On and Split Features from Labels\n",
        "\n",
        "Separate the target value (the \"label\") from the features. **This label is the value that you will train the model to predict.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2sluJdCW7jN"
      },
      "source": [
        "train_features = train_dataset[[\"Hippocampus\", \"WholeBrain\", \"Entorhinal\"]]\n",
        "test_features = test_dataset[[\"Hippocampus\", \"WholeBrain\", \"Entorhinal\"]]\n",
        "\n",
        "\n",
        "train_labels = train_dataset[\"AGE\"]\n",
        "test_labels = test_dataset[\"AGE\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRklxK5s388r"
      },
      "source": [
        "## 5. Normalization\n",
        "\n",
        "In the table of statistics it's easy to see how different the ranges of each feature are.\n",
        "\n",
        "Q: **(2 points)** Write in 100 words why normalization is important.\n",
        "\n",
        "*Note*: There is no advantage to normalizing the one-hot features, it is done here for simplicity. For more details on how to use the preprocessing layers, refer the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide and the [Classify structured data using Keras preprocessing layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers) tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcmY6lKKbkw8"
      },
      "source": [
        "train_features.describe().transpose()[['mean', 'std']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJ6ISropeoo"
      },
      "source": [
        "### 5.1 The Normalization Layer\n",
        "The `preprocessing.Normalization` layer is a clean and simple way to build that preprocessing into your model.\n",
        "\n",
        "The first step is to create the layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlC5ooJrgjQF"
      },
      "source": [
        "normalizer = preprocessing.Normalization(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYA2Ap6nVOha"
      },
      "source": [
        "Then `.adapt()` it to the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrBbbjbwV91f"
      },
      "source": [
        "normalizer.adapt(np.array(train_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZccMR5yV9YV"
      },
      "source": [
        "The following code calculates the mean and variance, and stores them in the layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGn-ukwxSPtx"
      },
      "source": [
        "print(normalizer.mean.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGWKaF9GSRuN"
      },
      "source": [
        "When the layer is called, it returns the input data with each feature independently normalized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l7zFL_XWIRu"
      },
      "source": [
        "first = np.array(train_features[:1])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "  print('First example:', first)\n",
        "  print()\n",
        "  print('Normalized:', normalizer(first).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o3CrycBXA2s"
      },
      "source": [
        "## 6. Linear regression\n",
        "\n",
        "Before building a DNN model, start with a linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFby9n0tnHkw"
      },
      "source": [
        "### 6.1. One Variable\n",
        "\n",
        "Start with a single-variable linear regression, to predict `AGE` from `hippocampus`.\n",
        "\n",
        "Q: **(5 points)** Please pick different variable or lable to explore the relationship of the features. \n",
        "\n",
        "Try and show at least 3 different variations.\n",
        "Training a model with `tf.keras` typically starts by defining the model architecture.\n",
        "\n",
        "In this case use a `keras.Sequential` model. This model represents a sequence of steps. In this case there are two steps:\n",
        "\n",
        "* Normalize the input `hippocampus`.\n",
        "* Apply a linear transformation ($y = mx+b$) to produce 1 output using `layers.Dense`.\n",
        "\n",
        "The number of _inputs_ can either be set by the `input_shape` argument, or automatically when the model is run for the first time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3gAFn3TPv8"
      },
      "source": [
        "First create the hippocampus `Normalization` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gJAy0fKs1TS"
      },
      "source": [
        "hippocampus = np.array(train_features['Hippocampus'])\n",
        "\n",
        "\n",
        "hippocampus_normalizer = preprocessing.Normalization(input_shape=[1,], axis=None)\n",
        "hippocampus_normalizer.adapt(hippocampus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NVlHJY2TWlC"
      },
      "source": [
        "Build the sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0sXM7qLlKfZ"
      },
      "source": [
        "hippocampus_model = tf.keras.Sequential([\n",
        "    hippocampus_normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "hippocampus_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObQu9fDnXGL"
      },
      "source": [
        "This model will predict `AGE` from `hippocampus`.\n",
        "\n",
        "Run the untrained model on the first 10 horse-power values. The output won't be good, but you'll see that it has the expected shape, `(10,1)`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfV1HS6bns-s"
      },
      "source": [
        "hippocampus_model.predict(hippocampus[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkanJlmmFBX"
      },
      "source": [
        "Once the model is built, configure the training procedure using the `Model.compile()` method. The most important arguments to compile are the `loss` and the `optimizer` since these define what will be optimized (`mean_absolute_error`) and how (using the `optimizers.Adam`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxA_3lpOm-SK"
      },
      "source": [
        "hippocampus_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3q1I9TwnRSC"
      },
      "source": [
        "Once the training is configured, use `Model.fit()` to execute the training:\n",
        "\n",
        "Q: **(5 points)** Explore different hyperparameters such as learning rate, epochs, batch sizes. Please document your explorations and reflections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iSrNy59nRAp"
      },
      "source": [
        "%%time\n",
        "history = hippocampus_model.fit(\n",
        "    train_features['Hippocampus'], train_labels,\n",
        "    epochs=100,\n",
        "    # suppress logging\n",
        "    verbose=1,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQm3pc0FYPQB"
      },
      "source": [
        "Visualize the model's training progress using the stats stored in the `history` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCAwD_y4AdC3"
      },
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E54UoZunqhc"
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 100])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYsQYrIZyqjz"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMNrt8X2ebXd"
      },
      "source": [
        "Collect the results on the test set, for later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDZ8EvNYrDtx"
      },
      "source": [
        "test_results = {}\n",
        "\n",
        "test_results['hippocampus_model'] = hippocampus_model.evaluate(\n",
        "    test_features['Hippocampus'],\n",
        "    test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0qutYAKwoda"
      },
      "source": [
        "Since this is a single variable regression it's easy to look at the model's predictions as a function of the input:\n",
        "\n",
        "Q: **(1 point)** Replace the hard-coded constants with the min & max of this variable to work with other variables without changing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDS2JEtOn9Jn"
      },
      "source": [
        "x = tf.linspace(3000, 11000, 100)\n",
        "y = hippocampus_model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x00t3uv2UhrV"
      },
      "source": [
        "Q: **(1 point)** The name of feature 'Hippocampus' and the label 'age' should be replaced as variables, so exploring different variables will be easy without making changes every time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rttFCTU8czsI"
      },
      "source": [
        "def plot_hippocampus(x, y):\n",
        "  plt.scatter(train_features['Hippocampus'], train_labels, label='Data')\n",
        "  plt.plot(x, y, color='k', label='Predictions')\n",
        "  plt.xlabel('Hippocampus')\n",
        "  plt.ylabel('AGE')\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l9ZiAOEUNBL"
      },
      "source": [
        "plot_hippocampus(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk2RmlqPoM9u"
      },
      "source": [
        "### 6.2. Multiple Variables (Features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PribnwDHUksC"
      },
      "source": [
        "You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector.\n",
        "\n",
        "This time use the `Normalization` layer that was adapted to the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssnVcKg7oMe6"
      },
      "source": [
        "linear_model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "linear_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHlx6WeIWyAr"
      },
      "source": [
        "When you call this model on a batch of inputs, it produces `units=1` outputs for each example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DynfJV18WiuT"
      },
      "source": [
        "linear_model.predict(train_features[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvHKH3rPXHmq"
      },
      "source": [
        "When you call the model it's weight matrices will be built. Now you can see that the `kernel` (the $m$ in $y=mx+b$) has a shape of `(9,1)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwJ4Fq0RXBQf"
      },
      "source": [
        "linear_model.layers[1].kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eINAc6rZXzOt"
      },
      "source": [
        "Use the same `compile` and `fit` calls as for the single input `hippocampus` model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Sv_Ybr0szp"
      },
      "source": [
        "linear_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZoOYORvoTSe"
      },
      "source": [
        "%%time\n",
        "history = linear_model.fit(\n",
        "    train_features, train_labels, \n",
        "    epochs=100,\n",
        "    # suppress logging\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data\n",
        "    validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdxiCbiNYK2F"
      },
      "source": [
        "Using all the inputs achieves a much lower training and validation error than the `hippocampus` model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sWO3W0koYgu"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyN49hIWe_NH"
      },
      "source": [
        "Collect the results on the test set, for later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNC3D1DGsGgK"
      },
      "source": [
        "test_results['linear_model'] = linear_model.evaluate(\n",
        "    test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3TLap5Ewnmf"
      },
      "source": [
        "test_predictions = linear_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [AGE]')\n",
        "plt.ylabel('Predictions [AGE]')\n",
        "lims = [60, 90]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORo1vWhDw64X"
      },
      "source": [
        "Q: **(6 points)** Explore different features as train_features and label and see what relationship help you understand the ADNI data set better. Show that you explored at least 3 different combinations of features and label and write your reflection in 200 words or more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmjdzxKzEu1-"
      },
      "source": [
        "# Extra Credit: A DNN regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_aHPsrzO1t"
      },
      "source": [
        "## Instructions\n",
        "\n",
        " DNN regression is for extra credit. You don't have to do the below parts. \n",
        " \n",
        " Q: **(Extra Credit = 3 points)** If you like to explore, then please go ahead and compare with the linear regression and write a reflection in 200 words.\n",
        "\n",
        "The previous section implemented linear models for single and multiple inputs.\n",
        "\n",
        "This section implements single-input and multiple-input DNN models. The code is basically the same except the model is expanded to include some \"hidden\" non-linear layers. The word \"hidden\" here just means not directly connected to the inputs or outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWtkIjhrZwa"
      },
      "source": [
        "These models will contain a few more layers than the linear model:\n",
        "\n",
        "* The normalization layer;\n",
        "* Two hidden, nonlinear, `Dense` layers using the `relu` nonlinearity; and\n",
        "* A linear single-output layer.\n",
        "\n",
        "Both will use the same training procedure so the `compile` method is included in the `build_and_compile_model` function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c26juK7ZG8j-"
      },
      "source": [
        "def build_and_compile_model(norm):\n",
        "  model = keras.Sequential([\n",
        "      norm,\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELz48lsgqC46"
      },
      "source": [
        "## A. Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T4RP1V36gVn"
      },
      "source": [
        "### A1. One Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvu9gtxTZR5V"
      },
      "source": [
        "Start with a DNN model for a single input, \"hippocampus\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGbPb-PHGbhs"
      },
      "source": [
        "dnn_hippocampus_model = build_and_compile_model(hippocampus_normalizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj49Og4YGULr"
      },
      "source": [
        "This model has quite a few more trainable parameters than the linear models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReAD0n6MsFK-"
      },
      "source": [
        "dnn_hippocampus_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-qWCsh6DlyH"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD7qHCmNIOY0"
      },
      "source": [
        "%%time\n",
        "history = dnn_hippocampus_model.fit(\n",
        "    train_features['Hippocampus'], train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dArGGxHxcKjN"
      },
      "source": [
        "This model does slightly better than the linear-hippocampus model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcF6UWjdCU8T"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG1snlpR2QCK"
      },
      "source": [
        "If you plot the predictions as a function of `hippocampus`, you'll see how this model takes advantage of the nonlinearity provided by the hidden layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPF53Rem14NS"
      },
      "source": [
        "x = tf.linspace(3000.0, 11000, 100)\n",
        "y = dnn_hippocampus_model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsf9rD8I17Wq"
      },
      "source": [
        "plot_hippocampus(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxCJKIUpe4io"
      },
      "source": [
        "Collect the results on the test set, for later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJjM0dU52XtN"
      },
      "source": [
        "test_results['dnn_hippocampus_model'] = dnn_hippocampus_model.evaluate(\n",
        "    test_features['Hippocampus'], test_labels,\n",
        "    verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_2Btebp2e64"
      },
      "source": [
        "### A2. Full Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKFtezDldLSf"
      },
      "source": [
        "If you repeat this process using all the inputs it slightly improves the performance on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0mhscXh2k36"
      },
      "source": [
        "dnn_model = build_and_compile_model(normalizer)\n",
        "dnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDENACl2tuW"
      },
      "source": [
        "%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features, train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9Dbj0fX23RQ"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWoVYS34fJPZ"
      },
      "source": [
        "Collect the results on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bZIa96W3c7K"
      },
      "source": [
        "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiCucdPLfMkZ"
      },
      "source": [
        "## B. Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDf1xebEfWBw"
      },
      "source": [
        "Now that all the models are trained check the test-set performance and see how they did:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5_ooufM5iH2"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [AGE]']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DABIVzsCf-QI"
      },
      "source": [
        "These results match the validation error seen during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft603OzXuEZC"
      },
      "source": [
        "## C. Make Predictions\n",
        "\n",
        "Finally, predict have a look at the errors made by the model when making predictions on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "source": [
        "test_predictions = dnn_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [AGE]')\n",
        "plt.ylabel('Predictions [AGE]')\n",
        "lims = [60, 90]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19wyogbOSU5t"
      },
      "source": [
        "It looks like the model predicts reasonably well. \n",
        "\n",
        "Now take a look at the error distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-OHX4DiXd8x"
      },
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel('Prediction Error [MPG]')\n",
        "_ = plt.ylabel('Count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSyaHUfDT-mZ"
      },
      "source": [
        "If you're happy with the model save it for later use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-WwLlmfT-mb"
      },
      "source": [
        "dnn_model.save('dnn_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Benlnl8UT-me"
      },
      "source": [
        "If you reload the model, it gives identical output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyyyj2zVT-mf"
      },
      "source": [
        "reloaded = tf.keras.models.load_model('dnn_model')\n",
        "\n",
        "test_results['reloaded'] = reloaded.evaluate(\n",
        "    test_features, test_labels, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_GchJ2tg-2o"
      },
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgGQuV-yqYZH"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:\n",
        "\n",
        "* [Mean Squared Error (MSE)](https://www.tensorflow.org/api_docs/python/tf/losses/MeanSquaredError) and [Mean Absolute Error (MAE)](https://www.tensorflow.org/api_docs/python/tf/losses/MeanAbsoluteError) are common loss functions used for regression problems. Mean Absolute Error is less sensitive to outliers. Different loss functions are used for classification problems.\n",
        "* Similarly, evaluation metrics used for regression differ from classification.\n",
        "* When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n",
        "* Overfitting is a common problem for DNN models, it wasn't a problem for this tutorial. See the [overfit and underfit](overfit_and_underfit.ipynb) tutorial for more help with this.\n"
      ]
    }
  ]
}